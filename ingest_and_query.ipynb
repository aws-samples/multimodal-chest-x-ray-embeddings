{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60087fb5",
   "metadata": {},
   "source": [
    "## Create an Index in OpenSearch Serverless Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '[COLLECTION_ID].us-east-1.aoss.amazonaws.com'  # serverless collection endpoint, without https://"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opensearch-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac3b63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import boto3\n",
    "\n",
    "region = 'us-east-1'  # e.g. us-east-1\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "# create an opensearch client and use the request-signer\n",
    "client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    pool_maxsize=20,\n",
    ")\n",
    "\n",
    "# create an index\n",
    "index_name = 'x-ray-image-embedding-vector-index'\n",
    "\n",
    "embedding_dim=384\n",
    "\n",
    "index_body = {\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"knn\": True\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": { #how do we store, \n",
    "    \"properties\": {\n",
    "        \"embedding\": {\n",
    "          \"type\": \"knn_vector\", #we are going to put \n",
    "          \"dimension\": embedding_dim,\n",
    "          \"method\": {\n",
    "            \"name\": \"hnsw\",\n",
    "            \"space_type\": \"l2\",\n",
    "            \"engine\": \"nmslib\",\n",
    "            \"parameters\": {\n",
    "              \"ef_construction\": 128,\n",
    "              \"m\": 24\n",
    "            }\n",
    "         }\n",
    "     }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = client.indices.create(index=index_name, body=index_body)\n",
    "\n",
    "print('\\nCreating index:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8dccbc",
   "metadata": {},
   "source": [
    "## Ingest Multimodal Embeddings into OpenSearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "\n",
    "# calls Amazon Bedrock to get a vector from either an image, text, or both\n",
    "def get_multimodal_vector(input_image_base64=None, input_text=None):\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "    request_body = {}\n",
    "    if input_text:\n",
    "        request_body[\"inputText\"] = input_text\n",
    "    if input_image_base64:\n",
    "        request_body[\"inputImage\"] = input_image_base64\n",
    "    request_body[\"embeddingConfig\"] = {\"outputEmbeddingLength\": 384}\n",
    "    body = json.dumps(request_body)\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body, \n",
    "        modelId=\"amazon.titan-embed-image-v1\", \n",
    "        accept=\"application/json\", \n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return embedding\n",
    "\n",
    "# creates a vector from an image file path\n",
    "def get_vector_from_file(file_path, label=None):\n",
    "    with open(file_path, \"rb\") as image_file:\n",
    "        input_image_base64 = base64.b64encode(image_file.read()).decode('utf8')    \n",
    "    vector = get_multimodal_vector(input_image_base64 = input_image_base64, input_text=label)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536900ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('./miccai2023_nih-cxr-lt_labels_train.csv') as input1, open('./miccai2023_nih-cxr-lt_labels_test.csv') as input2, open('./miccai2023_nih-cxr-lt_labels_val.csv') as input3, open(\"./labels.json\", \"w\") as output:\n",
    "    reader1 = csv.reader(input1)\n",
    "    reader2 = csv.reader(input2)\n",
    "    reader3 = csv.reader(input3)\n",
    "\n",
    "    res={}\n",
    "\n",
    "    i=0\n",
    "    for row in reader1:\n",
    "        if i==0:\n",
    "            categories=row\n",
    "            i+=1\n",
    "        else:\n",
    "            caption=''\n",
    "            for j, col in enumerate(row): \n",
    "                if j==0:\n",
    "                    key=col\n",
    "                else:\n",
    "                    if col=='1':\n",
    "                        caption+=categories[j]+' '\n",
    "\n",
    "            res[key]=caption.strip()\n",
    "            i+=1\n",
    "\n",
    "    i=0\n",
    "    for row in reader2:\n",
    "        if i==0:\n",
    "            categories=row\n",
    "            i+=1\n",
    "        else:\n",
    "            caption=''\n",
    "            for j, col in enumerate(row): \n",
    "                if j==0:\n",
    "                    key=col\n",
    "                else:\n",
    "                    if col=='1':\n",
    "                        caption+=categories[j]+' '\n",
    "\n",
    "            res[key]=caption.strip()\n",
    "            i+=1\n",
    "\n",
    "    i=0\n",
    "    for row in reader3:\n",
    "        if i==0:\n",
    "            categories=row\n",
    "            i+=1\n",
    "        else:\n",
    "            caption=''\n",
    "            for j, col in enumerate(row): \n",
    "                if j==0:\n",
    "                    key=col\n",
    "                else:\n",
    "                    if col=='1':\n",
    "                        caption+=categories[j]+' '\n",
    "\n",
    "            res[key]=caption.strip()\n",
    "            i+=1\n",
    "\n",
    "    output.write(json.dumps(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca70acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('labels.json') as json_file:\n",
    "    labels = json.load(json_file)\n",
    "\n",
    "    # Print the type of data variable\n",
    "    print(\"Type:\", type(labels))\n",
    "    \n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2c061",
   "metadata": {},
   "source": [
    "Below Step takes about 10 minutes, you can save time by decreasing the sample_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b071adb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Vectorize dataset and load it to OpenSearch\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "image_folder = 'images/images'\n",
    "\n",
    "image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
    "\n",
    "sample_size=1000\n",
    "\n",
    "i=0\n",
    "\n",
    "for image_file in image_files:\n",
    "    \n",
    "    if i<sample_size:\n",
    "        # Construct the full path to the image file\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "\n",
    "        if image_file in labels:\n",
    "            label=labels[image_file]\n",
    "        else:\n",
    "            label=''\n",
    "\n",
    "        img_embedding = get_vector_from_file(image_path, label)\n",
    "\n",
    "        image_document = {\n",
    "            'filename': image_file,\n",
    "            'diagnoses': label,\n",
    "            'embedding': img_embedding\n",
    "        }\n",
    "\n",
    "        response = client.index(\n",
    "            index = index_name,\n",
    "            body = image_document\n",
    "        )\n",
    "\n",
    "        print(f\"Inserted: {image_file}\")\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    else:\n",
    "        break\n",
    "    \n",
    "\n",
    "print(\"All images inserted into OpenSearch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4348e",
   "metadata": {},
   "source": [
    "## Query OpenSearch index using image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135234bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed query testing for an image\n",
    "query_embedding = get_vector_from_file('images/images/00000001_000.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_body = {\n",
    "    \"query\": {\"knn\": {\"embedding\": {\"vector\": query_embedding, \"k\": 3}}},\n",
    "    \"_source\": False,\n",
    "    \"fields\": [\"filename\", \"diagnoses\"],\n",
    "}\n",
    "\n",
    "results = client.search(\n",
    "    body=query_body,\n",
    "    index=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df273587",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hit in results['hits']['hits']:\n",
    "    \n",
    "    print(hit['_score'])\n",
    "    print(hit['fields']['diagnoses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597500d7",
   "metadata": {},
   "source": [
    "## [Optional] Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7b5f2",
   "metadata": {},
   "source": [
    "For above experiment we used the off-the-shelf version of titan multimodal embedding model to create the image embedding when querying the opensearch for similar images. What do you think the performance will be if we used the fine tuned model? Give it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648ee33",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the index\n",
    "delete_response = client.indices.delete(\n",
    "    index_name\n",
    ")\n",
    "\n",
    "print('\\nDeleting index:')\n",
    "print(delete_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
